{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Spark SQL\n",
    "\n",
    "In this notebook we will look at Spark SQL. Spark SQL lets us work on structured data. The notebook will follow the contents of [Spark Documentation](https://spark.apache.org/docs/2.2.1/sql-programming-guide.html). With the extra information Spark has for the structure of the data, some additional optimizations can be performed.\n",
    "\n",
    "\n",
    "#### Datasets and Dataframes\n",
    "\n",
    "Dataset is a distrbuted collection of data. Dataset can be constructed from JVM objects and comes with the benefits of Strong typing and ability to use lambda functions of RDDs along with the optimization advantages of Spark SQL.\n",
    "\n",
    "Dataframe is conceptually equivalent of a database table and has richer optimization under the hood. In Scala ``DataFrame`` is an alias of ``Dataset[Row]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = new org.apache.spark.sql.SQLContext(sc).sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create an instance of ``org.apache.spark.sql.SparkSession`` from the available ``sc`` object of ``org.apache.spark.SparkContext``. To that that we need to instantiate ``org.apache.spark.sql.SQLContext`` from with the current ``sc`` variable and then get the ``sparkSession``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val peopledf = spark.read.json(\"people.json\")\n",
    "peopledf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we saw above is loaded froma JSON file with contents \n",
    "\n",
    "```\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n",
    "\n",
    "```\n",
    "\n",
    "The file was downloaded from [this](https://github.com/apache/spark/blob/master/examples/src/main/resources/people.json) URL. The ``SparkSession`` instance was used to create a ``DataFrame`` instance from this JSON file and an appropriate type was inferred on loading the content. To view the schema of the `Dataframe` we do the following and we see that age is a numeric type and name is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopledf.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All operations on ``DataFrame`` are untyped and they dont fail until we execute them. We will see some examples below of perfectly valid operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "peopledf.select(\"name\").show()\n",
    "peopledf.select($\"name\", $\"age\" + 1).show()\n",
    "peopledf.filter($\"age\" > 20).show()\n",
    "peopledf.groupBy($\"age\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By importing ``import spark.implicits._`` we get to use ``$``. This is a convenient way to convert a string to a type ``org.apache.spark.sql.ColumnName``. The following two piece code fragments give the same results\n",
    "\n",
    "``peopledf.filter(new org.apache.spark.sql.ColumnName(\"age\") > 20).show()``\n",
    "\n",
    "and\n",
    "\n",
    "``peopledf.filter($\"age\" > 20).show()``\n",
    "\n",
    "no prize for guessing which is more readable, especially if we want to specify multiple column names.\n",
    "\n",
    "The ``select`` and ``filter`` operations returns us another``DataFrame``, however ``groupBy`` returns an object ``org.apache.spark.sql.RelationalGroupedDataset`` which further provides more aggregation operations like ``mean``, ``min``, ``max``, ``sum``, ``count``, ``pivot`` etc. \n",
    "\n",
    "The operations on ``DataFrames`` is not type safe and allows us to perfrom operations on types which doesn't make sense and gives unexpected results or even errors at run time. Following two examples, first where we compare a string value to be greater than a number giving us no results and second, we select a non existant field throwing a runtime exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`test`' given input columns: [age, name];;\n",
       "'Project ['test]\n",
       "+- AnalysisBarrier\n",
       "      +- Relation[age#26L,name#27] json\n",
       "\n",
       "StackTrace: 'Project ['test]\n",
       "+- AnalysisBarrier\n",
       "      +- Relation[age#26L,name#27] json\n",
       "\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3295)\n",
       "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1307)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopledf.filter($\"name\" > 20).show()\n",
    "peopledf.select($\"test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Running SQL Queries\n",
    "\n",
    "We can run sql like queries on ``DataFrames`` as follows. We can register a dataframe as a view either in the session or a global temporart view which can be used to query the data as follows.\n",
    "\n",
    "Notive how we prefix the ``global_temp.`` to the name of the view like we do for a schema of a table in a relational DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//A view local to session\n",
    "peopledf.createOrReplaceTempView(\"PeopleTemp\")\n",
    "\n",
    "//A  global temp view\n",
    "peopledf.createOrReplaceGlobalTempView(\"PeopleGlobal\")\n",
    "\n",
    "spark.sql(\"select * from PeopleTemp\").show()\n",
    "\n",
    "spark.sql(\"select * from global_temp.PeopleGlobal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now see how to create ``Dataset`` We do the following \n",
    "\n",
    "- Create a case class\n",
    "- Convert a sequence of case class instance to a ``Dataset``\n",
    "- show the contents of a ``Dataset``\n",
    "- Print the schema of the ``Dataset``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Andy| 32|\n",
      "+----+---+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "case class Person(name: String, age: Long)\n",
    "\n",
    "val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n",
    "\n",
    "caseClassDS.show()\n",
    "\n",
    "caseClassDS.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A ``DataFrame`` can be converted to a ``Dataset`` as follows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val personDS = spark.read.json(\"people.json\").as[Person]\n",
    "\n",
    "personDS.show()\n",
    "\n",
    "personDS.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will see how we can interoperate between SQL and programatic API of Spark. Notice though that when we use ``DataFrames`` we no longer have strong typing and need to know the structure of the data when we use the programatic approach. The show function accepts a parameter ``truncate`` and we set it to false to show the entire content of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n",
      "+--------------------+\n",
      "|value               |\n",
      "+--------------------+\n",
      "|Name: Justin,Age: 19|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val teenageDF = spark.sql(\"select * from PeopleTemp where age between 13 and 19\")\n",
    "\n",
    "teenageDF.show()\n",
    "//Map names\n",
    "\n",
    "teenageDF.map(teenage => \"Name: \" + teenage(1) + \", Age: \" + teenage(0)).show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Aggregations\n",
    "\n",
    "We will now see how we can perform aggregations on ``DataFrame`` and ``Dataset``\n",
    "\n",
    "\n",
    "For these examples we will use ``employees.json`` downloaded from [thus](https://raw.githubusercontent.com/apache/spark/master/examples/src/main/resources/employees.json) URL. \n",
    "\n",
    "First we will create a ``DataFrame`` from thus data file and then find the mean as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+\n",
      "|MeanSalary|\n",
      "+----------+\n",
      "|    3750.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val employeeDF = spark.read.json(\"employees.json\")\n",
    "employeeDF.show()\n",
    "employeeDF.printSchema\n",
    "employeeDF.createOrReplaceTempView(\"Employees\")\n",
    "spark.sql(\"select mean(salary) as MeanSalary from Employees\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But what if we want to write our own aggregation function that we want to use? This is similar to the aggregate or reduce call we will make on an RDD but is bit complicated. We will see step by step what we need to do \n",
    "\n",
    "- create an object that extends from ``org.apache.spark.sql.expressions.UserDefinedAggregateFunction``\n",
    "- There are several abstract methods we need to implement. We will see what they mean and what they are. As an example we will implement the same mean function we will call it ``myMean``.\n",
    "\n",
    "    - We start with what is the input to the function. In our case of myMean, the input type is a number.We then start by implementing the method ``inputSchema`` which returns a field ``StructType`` which tells us the data type of the input field(s). In our case it will be one field of the ``DataFrame`` and that frame will be of type ``Long``    \n",
    "    - Then there is a buffer, which will hold the values when the aggregation is being done. In case of ``myMean`` we need to hold two values, the running total and the number of rows we visited. We thus implement our next method ``bufferSchema`` which will again return a ``StructType``, but this time the returned datatype has two fields, both of type Long to hold the running total and running count.\n",
    "    - There is another datatype which is the datatype of the return type of the ``myMean`` function. In this case it will be Long type of data and this is represented by yet another ``StructType`` and is implemented by the field ``dataType``.\n",
    "    - There is a method called ``deterministic`` which we implement to return true or false which essentially tells us if the value always same for a given set of inputs. This information probably is used by Spark to use cached values in case the given input was already seen and computed and the value is deterministic.\n",
    "    - We then initialize the buffer to the starting value. In our case for mean, both running total and number of records processed is set to 0. The method to be implemented is called ``initialize``\n",
    "    - We need to update the buffer with a new record. In case if ``myMean`` we simply increment the record count in the buffer by one and add the running total with the value we have for the current record. This is done by implenting the ``update`` method.\n",
    "    - The ``DataFrame`` is distributed and the aggregation happens on various partitions in parallel. There needs to be a way to merge two buffers into one. In our case of we simply add the running total and number of records processed from both buffers in one buffer. This is done by implementing the method ``merge``.\n",
    "    - Finally we need to evaluate the final value using the accumulated buffer. In our case, we divide the  running total with the total number of records. This is done by implementing the ``evaluate`` function.\n",
    "\n",
    "\n",
    "Once this object is implemented, simply register it with spark session as a udf (Use Defined Function) with a given name. Once this is done we can start using it in the query functions like any other inbuilt function like mean.\n",
    "\n",
    "Lets see how we implement this ``myMean`` in Spark.\n",
    "\n",
    "TODO: Show the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Spark SQL\n",
    "\n",
    "In this notebook we will look at Spark SQL. Spark SQL lets us work on structured data. The notebook will follow the contents of [Spark Documentation](https://spark.apache.org/docs/2.2.1/sql-programming-guide.html). With the extra information Spark has for the structure of the data, some additional optimizations can be performed.\n",
    "\n",
    "\n",
    "#### Datasets and Dataframes\n",
    "\n",
    "Dataset is a distrbuted collection of data. Dataset can be constructed from JVM objects and comes with the benefits of Strong typing and ability to use lambda functions of RDDs along with the optimization advantages of Spark SQL.\n",
    "\n",
    "Dataframe is conceptually equivalent of a database table and has richer optimization under the hood. In Scala ``DataFrame`` is an alias of ``Dataset[Row]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = new org.apache.spark.sql.SQLContext(sc).sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create an instance of ``org.apache.spark.sql.SparkSession`` from the available ``sc`` object of ``org.apache.spark.SparkContext``. To that that we need to instantiate ``org.apache.spark.sql.SQLContext`` from with the current ``sc`` variable and then get the ``sparkSession``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val peopledf = spark.read.json(\"people.json\")\n",
    "peopledf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we saw above is loaded froma JSON file with contents \n",
    "\n",
    "```\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n",
    "\n",
    "```\n",
    "\n",
    "The file was downloaded from [this](https://github.com/apache/spark/blob/master/examples/src/main/resources/people.json) URL. The ``SparkSession`` instance was used to create a ``DataFrame`` instance from this JSON file and an appropriate type was inferred on loading the content. To view the schema of the `Dataframe` we do the following and we see that age is a numeric type and name is a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopledf.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All operations on ``DataFrame`` are untyped and they dont fail until we execute them. We will see some examples below of perfectly valid operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "peopledf.select(\"name\").show()\n",
    "peopledf.select($\"name\", $\"age\" + 1).show()\n",
    "peopledf.filter($\"age\" > 20).show()\n",
    "peopledf.groupBy($\"age\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By importing ``import spark.implicits._`` we get to use ``$``. This is a convenient way to convert a string to a type ``org.apache.spark.sql.ColumnName``. The following two piece code fragments give the same results\n",
    "\n",
    "``peopledf.filter(new org.apache.spark.sql.ColumnName(\"age\") > 20).show()``\n",
    "\n",
    "and\n",
    "\n",
    "``peopledf.filter($\"age\" > 20).show()``\n",
    "\n",
    "no prize for guessing which is more readable, especially if we want to specify multiple column names.\n",
    "\n",
    "The ``select`` and ``filter`` operations returns us another``DataFrame``, however ``groupBy`` returns an object ``org.apache.spark.sql.RelationalGroupedDataset`` which further provides more aggregation operations like ``mean``, ``min``, ``max``, ``sum``, ``count``, ``pivot`` etc. \n",
    "\n",
    "The operations on ``DataFrames`` is not type safe and allows us to perfrom operations on types which doesn't make sense and gives unexpected results or even errors at run time. Following two examples, first where we compare a string value to be greater than a number giving us no results and second, we select a non existant field throwing a runtime exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: cannot resolve '`test`' given input columns: [age, name];;\n",
       "'Project ['test]\n",
       "+- AnalysisBarrier\n",
       "      +- Relation[age#26L,name#27] json\n",
       "\n",
       "StackTrace: 'Project ['test]\n",
       "+- AnalysisBarrier\n",
       "      +- Relation[age#26L,name#27] json\n",
       "\n",
       "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n",
       "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:120)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n",
       "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:120)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\n",
       "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n",
       "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n",
       "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n",
       "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n",
       "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3295)\n",
       "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1307)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopledf.filter($\"name\" > 20).show()\n",
    "peopledf.select($\"test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Spark ML\n",
    "\n",
    "This notebook will introduce Spark ML and its API.\n",
    "\n",
    "#### Correlation calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"SparkML\").getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "1.0                   0.055641488407465814  NaN  0.4004714203168137  \n",
      "0.055641488407465814  1.0                   NaN  0.9135958615342522  \n",
      "NaN                   NaN                   1.0  NaN                 \n",
      "0.4004714203168137    0.9135958615342522    NaN  1.0                 \n",
      "\n",
      "Spearman correlation matrix:\n",
      "1.0                  0.10540925533894532  NaN  0.40000000000000174  \n",
      "0.10540925533894532  1.0                  NaN  0.9486832980505141   \n",
      "NaN                  NaN                  1.0  NaN                  \n",
      "0.40000000000000174  0.9486832980505141   NaN  1.0                  \n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.ml.linalg.{Vectors, Matrix}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "  Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),  \n",
    "  Vectors.dense(4.0, 5.0, 0.0, 3.0),\n",
    "  Vectors.dense(6.0, 7.0, 0.0, 8.0),\n",
    "  Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))\n",
    ")\n",
    "\n",
    "val df = data.map(Tuple1.apply).toDF(\"features\")\n",
    "val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\n",
    "println(\"Pearson correlation matrix:\\n\" + coeff1.toString)\n",
    "val Row(coeff2: Matrix) = Correlation.corr(df, \"features\", \"spearman\").head\n",
    "println(\"\\nSpearman correlation matrix:\\n\" + coeff2.toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let us understand the above piece of code step by step. There are two ways to create vectors, first one is dense and another is sparse. In a dense vector we specify all n elements and its values. The dense vector is simply created as ``Vectors.dense(4.0, 5.0, 0.0, 3.0)`` where as sparse is created as ``Vectors.sparse(4, Seq((0, 1.0), (3, -2.0)))`` wgere the first number if the number of elements/dimension in the vector, and the ``Seq`` given is a tuple of index, (value pair). Thus ``Vectors.sparse(4, Seq((0, 1.0), (3, -2.0)))`` is same as ``Vectors.dense(1.0, 0.0, 0.0, -2.0)`` as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,0.0,0.0,-2.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))).toDense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The correlation matrix is is always symmetric across diagonal with diagonal values being 1 as the corelation of a vector with itself is always 1. The matrix is symmetric across the diagonal that is element (0, 1) is same as (1, 0), (0, 2) same as (2, 0) and so on.\n",
    "\n",
    "The formula for pearson correlation is \n",
    "\n",
    "$p\\:=\\:\\frac{n\\sum{xy} - (\\sum{x})(\\sum{y})}{\\sqrt{[n\\sum{x^2} - (\\sum{x})^2][n\\sum{y^2} - (\\sum{y})^2]}}$\n",
    "\n",
    "The correlation between two lists (1.0, 4.0, 6.0, 9.0) and (0.0, 5.0, 7.0, 0.0)\n",
    "as per [this](http://calculator.vhex.net/calculator/statistics/pearson-correlation) URL is expected to be 0.055641.\n",
    "\n",
    "The following code snippet calculates this pearson correlation between two list of doubles of equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson coefficient between v1 and v2 is 0.055641488407465724\n"
     ]
    }
   ],
   "source": [
    "import scala.math.sqrt\n",
    "\n",
    "def pearsonCorrelation(x:List[Double], y: List[Double]): Double = {\n",
    "    val n = x.length\n",
    "    val sumx = x.reduce(_ + _)\n",
    "    val sumxsquare = x.map(e => e * e).reduce(_ + _)\n",
    "    val sumy = y.reduce(_ + _)\n",
    "    val sumysquare = y.map(e => e * e).reduce(_ + _)\n",
    "    val sumxy = (x zip y).map{case (l, r) => l * r}.reduce(_ + _)\n",
    "    val numerator = (n * sumxy) - (sumx * sumy)\n",
    "    val denominator = (n * sumxsquare - sumx * sumx) * (n * sumysquare - sumy * sumy)\n",
    "    numerator / math.sqrt(denominator)\n",
    "}\n",
    "\n",
    "val v1 = List(1.0, 4.0, 6.0, 9.0)\n",
    "val v2 = List(0, 5.0, 7.0, 0)\n",
    "println(\"Pearson coefficient between v1 and v2 is \" + pearsonCorrelation(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance between x and y is 0.055641488407465724\n"
     ]
    }
   ],
   "source": [
    "// Alternate implementation of above but by calculating the mean value first. \n",
    "// The above implementation doesn't need to calculate\n",
    "val n = v1.length\n",
    "val meanx = v1.reduce(_ + _) * 1.0 / n\n",
    "val meany = v2.reduce(_ + _) * 1.0 / n\n",
    "\n",
    "val numerator = (v1 zip v2).map{\n",
    " case (e1, e2) => (e1 - meanx) * (e2 - meany)\n",
    "}.reduce(_ + _)\n",
    "\n",
    "val sumxsquare = v1.map( e => (e - meanx) * (e - meanx)).reduce(_ + _)\n",
    "val sumysquare= v2.map(e => (e - meany) * (e - meany)).reduce(_ + _)\n",
    "val denominator = math.sqrt(sumxsquare) * math.sqrt(sumysquare)\n",
    "\n",
    "\n",
    "println(\"Covariance between x and y is \" + numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The vectors defined in ``data`` for which we computed the pearson coefficient are\n",
    "\n",
    "```\n",
    "[1.0,0.0,0.0,-2.0]\n",
    "[4.0,5.0,0.0,3.0]\n",
    "[6.0,7.0,0.0,8.0]\n",
    "[9.0,0.0,0.0,1.0]\n",
    "```\n",
    "\n",
    "The line ``val df = data.map(Tuple1.apply).toDF(\"features\")`` creates a ``DataFrame``. The ``toDF(columnName)`` is a a function that can be applied to ``Seq[Tuple{n}]``. This is an implicit function that lets us convert tuples to data frame which we get by the import ``scala.implicits._``. Each tuple in the sequence becomes a row and each element of the tuple becomes a column. To achieve this, we need to convert ``Seq[Vector]`` to ``Seq[Tuple1]`` using ``map`` so that we can convert the ``Seq[Tuple1]`` to a ``DataFrame`` with one column which we will call features.\n",
    "\n",
    "The code ``Correlation.corr(df, \"features\")`` computes the correlation matrix. The return value of this call is a ``DataFrame`` with one row and one column. The name of the column is ``pearson(<nae of the original column in dataset>)`` and the type of the value is a ``Matrix``. The code ``val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head`` is a one liner to get the first and only row of this ``DataFrame`` and assign the value of the ``Matrix`` in it to the variable ``coeff1``.\n",
    "\n",
    "The parameters of the ``corr`` function are the ``DataFrame`` instance, the name of the column in the ``DataFrame`` and an optional third parameter for the correlation method, which defaults to ``pearson``. Only valid value for now is ``spearman``.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### ChiSquared Test\n",
    "\n",
    "We will now look at Hypothesis testing which tests whether the result we get is stastically significant or not. We will see Pearsons Chi-Squared tests in this section. First a code sample and the result we get\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the ChiSquaredTest DataFrame\n",
      "+---------------------------------------+----------------+----------+\n",
      "|pValues                                |degreesOfFreedom|statistics|\n",
      "+---------------------------------------+----------------+----------+\n",
      "|[0.6872892787909721,0.6822703303362126]|[2, 3]          |[0.75,1.5]|\n",
      "+---------------------------------------+----------------+----------+\n",
      "\n",
      "pValues = [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom = [2,3]\n",
      "statistics = [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "\n",
    "val data = Seq(\n",
    "  (0.0, Vectors.dense(0.5, 10.0)),\n",
    "  (0.0, Vectors.dense(1.5, 20.0)),\n",
    "  (1.0, Vectors.dense(1.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 40.0)),\n",
    "  (1.0, Vectors.dense(3.5, 40.0))\n",
    ")\n",
    "\n",
    "val df = data.toDF(\"label\", \"features\")\n",
    "val chiDF = ChiSquareTest.test(df, \"features\", \"label\")\n",
    "val chi = chiDF.head\n",
    "println(\"Printing the ChiSquaredTest DataFrame\")\n",
    "chiDF.show(truncate = false)\n",
    "println(\"pValues = \" + chi.getAs[Vector](0))\n",
    "println(\"degreesOfFreedom = \" + chi.getSeq[Int](1).mkString(\"[\", \",\", \"]\"))\n",
    "println(\"statistics = \" + chi.getAs[Vector](2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now derive the above statistics using plain RDDs to see how the calculation is performed step by step. Note that this is not necessarily the most efficient way, but it demonstrates the steps nevertheless. \n",
    "\n",
    "We start by creating a contingency matrix for the each feature. The number of rows of this matrix are same as the number of unique values of that feature and the number of columns is same as number of unique labels. The values in the matrix are same as the number of occurances for that feature, label combination. In our case we will build two contingency matrix for each feature. For the first feature the unique values are (0.5, 1.5, 3.5) and the second feature has the unique values (10, 20, 30, 40). The number of columns are 2 in both cases for the labels (1, 0).\n",
    "\n",
    "As an illustration, the matrix for first feature would be as follows. We have also added the row sum and the col sum for the following matrix\n",
    "\n",
    "|               | **0**          | **1**  | **sum**|\n",
    "|:-------------: |:-------------:|:-------------: |:-------------: |\n",
    "| **0.5**      |  1| 0 |1|\n",
    "| **1.5**      | 1      |   1 |2|\n",
    "| **3.5**      | 2     |    1 |3|\n",
    "| **sum**      | 4    |   2  ||\n",
    "\n",
    "\n",
    "Let us give the input as a List of ``Tuples[(Double, Double)]``. This is for one dimension of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Label, Feature) pairs counts are [(1.5,0) -> 1, (3.5,0) -> 2, (3.5,1) -> 1, (0.5,0) -> 1, (1.5,1) -> 1]\n",
      "Row Sum are [3.5 -> 3, 1.5 -> 2, 0.5 -> 1]\n",
      "Col Sum are [1 -> 2, 0 -> 4]\n"
     ]
    }
   ],
   "source": [
    "val inputList = List((0.5, 0), (1.5, 0), (1.5, 1), (3.5, 0), (3.5, 0), (3.5, 1))\n",
    "val inputs = sc.parallelize(inputList)\n",
    "val freq = inputs.map(i => (i, 1)).reduceByKey(_ + _).collectAsMap\n",
    "val rowSum = inputs.map(i => (i._1, 1)).reduceByKey(_ + _).collectAsMap\n",
    "val colSum = inputs.map(i => (i._2, 1)).reduceByKey(_ + _).collectAsMap\n",
    "val inputSize = inputList.size\n",
    "val uniqueLabels = inputList.map(_._2).toSet\n",
    "val uniqueFeatures = inputList.map(_._1).toSet\n",
    "println(\"(Label, Feature) pairs counts are \" + \n",
    "                freq.mkString(\"[\", \", \", \"]\") + \n",
    "                \"\\nRow Sum are \" + rowSum.mkString(\"[\", \", \", \"]\") + \n",
    "                \"\\nCol Sum are \" + colSum.mkString(\"[\", \", \", \"]\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above Map, Row Sum and Col Sum values are same as the the Matrix we saw earlier. The next step is to compute the expected value. The way we compute the expected value is compute the probability of each of the possible labels and then multiply the total number of occurances of the feature with these probability.\n",
    "\n",
    "For example, in the above case, from Col Sum we have the frequency of label 0 and 1 is 0.67 (4 / 6) and 0.33 (2 / 6) respectively. Expected value for feature value 3.5 would be 2( 3 $\\times$ 0.67) and 1( 3 $\\times$ 0.33) for label 0 and 1 respectively\n",
    "\n",
    "Following code computes the expected value of each feature value, label value combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected values for possible (Label, Feature) pair are Set(((1.5,0),1.3333333333333333), ((3.5,0),2.0), ((0.5,1),0.3333333333333333), ((0.5,0),0.6666666666666666), ((1.5,1),0.6666666666666666), ((3.5,1),1.0))\n"
     ]
    }
   ],
   "source": [
    "val expectedValues = (for(f <- uniqueFeatures ; l <-  uniqueLabels) yield(f, l)) map {\n",
    "    case (f, l) => ((f, l), 1.0 * rowSum(f) * colSum(l) / inputSize)\n",
    "}\n",
    "println(\"Expected values for possible (Label, Feature) pair are \" + expectedValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For calculating $\\chi^2$ stat. We will implement the following\n",
    "\n",
    "$\\chi^2\\:=\\:\\sum_{i=1}^{N}\\frac{(O_i - E_i)^2}{E_i}$\n",
    "\n",
    "Where\n",
    "\n",
    "- $O_i$ = the number of observations of type i.\n",
    "- N = Total Number of observations\n",
    "- $E_i$ = Expected theoritical probability of type i\n",
    "\n",
    "\n",
    "Degrees of freedom is simply computed as $(n_f - 1) \\times (n_l - 1)$\n",
    "\n",
    "Where \n",
    "\n",
    "- $n_f$: Number of unique feature values\n",
    "- $n_l$: Number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat is 0.7500000000000001, degrees of freedom are 2\n"
     ]
    }
   ],
   "source": [
    "val stat = expectedValues.foldLeft(0.0){\n",
    "    case(acc, (k, e)) =>\n",
    "        val f = freq.getOrElse(k, 0)\n",
    "        val diff = f - e\n",
    "        acc + diff * diff / e        \n",
    "}\n",
    "val df = (uniqueFeatures.size - 1) * (uniqueLabels.size - 1)\n",
    "println(\"Stat is \" + stat + \", degrees of freedom are \" + df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "With the Stat and degrees of freedom calculated, we will now calculate the pValue. The above degrees of freedom and stat are in sync with what we calculated using SparkML.\n",
    "\n",
    "pValue calculation isnt straight forward and SparkML uses Apache Commons Math. We will do the same as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p value for the stat 0.7500000000000001 for 2 degrees of freedom is 0.6872892787909721\n"
     ]
    }
   ],
   "source": [
    "import org.apache.commons.math3.distribution.ChiSquaredDistribution\n",
    "println(\"The p value for the stat \" + stat + \" for \" + df + \" degrees of freedom is \" + \n",
    "        (1 - new ChiSquaredDistribution(df).cumulativeProbability(stat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ML Pipeline\n",
    "\n",
    "Spark ML standardizes the API for ML Algorithms to make it easy to use different algorithms in the same pipeline. Following are the important components\n",
    "\n",
    "- DataFrame: This is the same dataframe we saw in Spark SQL. The dataframe is used to store the dataset and can be used to store various features, text, feature vectors etc.\n",
    "- Transformer: This converts a ``DataFrame`` to another ``DataFrame``. ML model is a transformer which converts the input data frame or features to predictions. It can also be used for feature engineering where we use an existing DataFrame and add more engineered columns to the DataFrame.\n",
    "- Estimator: This takes in a dataframe and gives a model. That is, it takes in a DataFrame and gives a Transformer.The learning algorithm used is in fact an Estimator.\n",
    "- Pipeline: It chains multiple Estimators and Transformers to specify an ML Workflow.\n",
    "- Parameter: All Transformers and Estimators share a common API for specifying parameters.\n",
    "\n",
    "\n",
    "A Pipeline is a Transformer. Thus ``transform`` method is invoked on each component of the ``Pipeline`` to produce a ``DataFrame`` fed into the next component of the pipeline. If any component is an ``Estimator`` it calls the ``fit``method to produce a ``Model`` which is a ``Transformer``.\n",
    "\n",
    "---\n",
    "\n",
    "Let us now see how to train a model. We will use a ``LogisticRegression`` model with max 10 iterations and use 0.01 as $\\lambda$ for regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Vectors are\n",
      "\n",
      "+------+--------------+\n",
      "|Labels|Features      |\n",
      "+------+--------------+\n",
      "|1.0   |[0.0,1.1,0.1] |\n",
      "|0.0   |[2.0,1.0,-1.0]|\n",
      "|0.0   |[2.0,1.3,1.0] |\n",
      "|1.0   |[0.0,1.2,-0.5]|\n",
      "+------+--------------+\n",
      "\n",
      "Explanation of parameters for this Regression is \n",
      "\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features, current: Features)\n",
      "fitIntercept: whether to fit an intercept term (default: true)\n",
      "labelCol: label column name (default: label, current: Labels)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model (default: true)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "val inputVectors = spark.createDataFrame(Seq((1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    "  (0.0, Vectors.dense(2.0, 1.0, -1.0)),\n",
    "  (0.0, Vectors.dense(2.0, 1.3, 1.0)),\n",
    "  (1.0, Vectors.dense(0.0, 1.2, -0.5)))).toDF(\"Labels\", \"Features\")\n",
    "\n",
    "println(\"Input Vectors are\\n\") \n",
    "inputVectors.show(truncate = false)\n",
    "  \n",
    "val lr1 = new LogisticRegression()\n",
    "println(\"Explanation of parameters for this Regression is \\n\\n\" + lr.explainParams + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Calling ``explainParam`` on any ``Transformer`` will give us a documentation of all possible parameters the ``Transformer`` supports. Let us train the Model with the given input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was fit using parameters: {\n",
      "\tlogreg_0b047d778a82-aggregationDepth: 2,\n",
      "\tlogreg_0b047d778a82-elasticNetParam: 0.0,\n",
      "\tlogreg_0b047d778a82-family: auto,\n",
      "\tlogreg_0b047d778a82-featuresCol: Features,\n",
      "\tlogreg_0b047d778a82-fitIntercept: true,\n",
      "\tlogreg_0b047d778a82-labelCol: Labels,\n",
      "\tlogreg_0b047d778a82-maxIter: 10,\n",
      "\tlogreg_0b047d778a82-predictionCol: prediction,\n",
      "\tlogreg_0b047d778a82-probabilityCol: probability,\n",
      "\tlogreg_0b047d778a82-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_0b047d778a82-regParam: 0.01,\n",
      "\tlogreg_0b047d778a82-standardization: true,\n",
      "\tlogreg_0b047d778a82-threshold: 0.5,\n",
      "\tlogreg_0b047d778a82-tol: 1.0E-6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr.setMaxIter(10).setRegParam(0.01).setLabelCol(\"Labels\").setFeaturesCol(\"Features\")\n",
    "val model1 = lr.fit(inputVectors)\n",
    "println(\"Model was fit using parameters: \" + model1.extractParamMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, we train a Model with the input Model instance. We can configure it by chaining calls to set the required parameters. Following is an alternate way to create a similar model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was fit using parameters: {\n",
      "\tlogreg_1b9826f1f816-aggregationDepth: 2,\n",
      "\tlogreg_1b9826f1f816-elasticNetParam: 0.0,\n",
      "\tlogreg_1b9826f1f816-family: auto,\n",
      "\tlogreg_1b9826f1f816-featuresCol: Features,\n",
      "\tlogreg_1b9826f1f816-fitIntercept: true,\n",
      "\tlogreg_1b9826f1f816-labelCol: Labels,\n",
      "\tlogreg_1b9826f1f816-maxIter: 10,\n",
      "\tlogreg_1b9826f1f816-predictionCol: prediction,\n",
      "\tlogreg_1b9826f1f816-probabilityCol: probability,\n",
      "\tlogreg_1b9826f1f816-rawPredictionCol: rawPrediction,\n",
      "\tlogreg_1b9826f1f816-regParam: 0.01,\n",
      "\tlogreg_1b9826f1f816-standardization: true,\n",
      "\tlogreg_1b9826f1f816-threshold: 0.5,\n",
      "\tlogreg_1b9826f1f816-tol: 1.0E-6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.param.ParamMap\n",
    "\n",
    "val lr2 = new LogisticRegression()\n",
    "val pm = ParamMap(lr2.regParam -> 0.01, lr2.labelCol -> \"Labels\", lr2.featuresCol -> \"Features\", lr2.maxIter -> 10)\n",
    "val model2 = lr2.fit(inputVectors, pm)\n",
    "println(\"Model was fit using parameters: \" + model2.extractParamMap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Following code snippet will now use the Test model and give results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions are\n",
      "\n",
      "+--------------+--------------------------------------+------------------------------------------+----------+\n",
      "|Features      |rawPrediction                         |probability                               |prediction|\n",
      "+--------------+--------------------------------------+------------------------------------------+----------+\n",
      "|[-1.0,1.5,1.3]|[-6.587201443935503,6.587201443935503]|[0.0013759947069214356,0.9986240052930786]|1.0       |\n",
      "|[3.0,2.0,-0.1]|[3.980182819425658,-3.980182819425658]|[0.9816604009374171,0.018339599062582944] |0.0       |\n",
      "|[0.0,2.2,-1.5]|[-6.37651770286046,6.37651770286046]  |[0.0016981475578358373,0.9983018524421641]|1.0       |\n",
      "+--------------+--------------------------------------+------------------------------------------+----------+\n",
      "\n",
      "Inputs with the labels are\n",
      "\n",
      "+------+--------------+\n",
      "|Labels|Features      |\n",
      "+------+--------------+\n",
      "|1.0   |[-1.0,1.5,1.3]|\n",
      "|0.0   |[3.0,2.0,-0.1]|\n",
      "|1.0   |[0.0,2.2,-1.5]|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val test = spark.createDataFrame(Seq(\n",
    "  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),\n",
    "  (0.0, Vectors.dense(3.0, 2.0, -0.1)),\n",
    "  (1.0, Vectors.dense(0.0, 2.2, -1.5))\n",
    ")).toDF(\"Labels\", \"Features\")\n",
    "\n",
    "val res = model2.transform(test.select(\"Features\"))\n",
    "println(\"Predictions are\\n\")\n",
    "res.show(truncate = false)\n",
    "println(\"Inputs with the labels are\\n\")\n",
    "test.show(truncate = false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Sample Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val training = spark.createDataFrame(Seq(\n",
    "  (0L, \"a b c d e spark\", 1.0),\n",
    "  (1L, \"b d\", 0.0),\n",
    "  (2L, \"spark f g h\", 1.0),\n",
    "  (3L, \"hadoop mapreduce\", 0.0)\n",
    ")).toDF(\"id\", \"Text\", \"Label\")\n",
    "\n",
    "//1. Create Transformers to amend the inputDataFrame to add new columns, Tokens and tf_features\n",
    "val token = new Tokenizer().setInputCol(\"Text\").setOutputCol(\"Tokens\")\n",
    "\n",
    "val hash = new HashingTF().setInputCol(\"Tokens\").setOutputCol(\"tf_features\")\n",
    "\n",
    "val lr = new LogisticRegression().setFeaturesCol(\"tf_features\").setLabelCol(\"Label\").setMaxIter(10).setRegParam(0.01)\n",
    "\n",
    "val pipe = new Pipeline().setStages(Array(token, hash, lr))\n",
    "\n",
    "val pipelineModel = pipe.fit(training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the above code snippet we did the following\n",
    "\n",
    "- Create a training ``DataFrame``\n",
    "- Create a transformer ``token`` or type ``Tokenizer`` which tokanizes the input sentences. To see what possible values can be configured we can invoke ``token.explainParams``\n",
    "- Create a transformer ``hash`` which is of type ``HashingTF``. This transformer takes in word vectors and outputs the term frequency usinh Hashing. Again, ``hash.explainParams`` will give details of all the possible parameters.\n",
    "- Create a ``LogisticRegression`` instance.\n",
    "- Create a ``Pipeline`` which adds the three stages for tokenize, hashing and LogisticRegression in that order.\n",
    "- Fit the pipeline with the traning data which gives us a ``PipelineModel``. \n",
    "\n",
    "To see what exactly is output by the first two transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+----------------------+\n",
      "|id |Text            |Label|Tokens                |\n",
      "+---+----------------+-----+----------------------+\n",
      "|0  |a b c d e spark |1.0  |[a, b, c, d, e, spark]|\n",
      "|1  |b d             |0.0  |[b, d]                |\n",
      "|2  |spark f g h     |1.0  |[spark, f, g, h]      |\n",
      "|3  |hadoop mapreduce|0.0  |[hadoop, mapreduce]   |\n",
      "+---+----------------+-----+----------------------+\n",
      "\n",
      "+---+----------------------+--------------------------------------------------------------------------+\n",
      "|id |Tokens                |tf_features                                                               |\n",
      "+---+----------------------+--------------------------------------------------------------------------+\n",
      "|0  |[a, b, c, d, e, spark]|(262144,[17222,27526,28698,30913,227410,234657],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|1  |[b, d]                |(262144,[27526,30913],[1.0,1.0])                                          |\n",
      "|2  |[spark, f, g, h]      |(262144,[15554,24152,51505,234657],[1.0,1.0,1.0,1.0])                     |\n",
      "|3  |[hadoop, mapreduce]   |(262144,[42633,155117],[1.0,1.0])                                         |\n",
      "+---+----------------------+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val tokenized = token.transform(training)\n",
    "tokenized.show(truncate = false)\n",
    "\n",
    "val hashed = hash.transform(tokenized)\n",
    "hashed.select(\"id\", \"Tokens\", \"tf_features\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now that we have the ``Pipeline`` and the ``PipelineModel``. Let us test with our sample input data in the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------------------+----------+\n",
      "|id |Text              |probability                              |prediction|\n",
      "+---+------------------+-----------------------------------------+----------+\n",
      "|4  |spark i j k       |[0.5406433544852275,0.45935664551477245] |0.0       |\n",
      "|5  |l m n             |[0.933438262738352,0.06656173726164803]  |0.0       |\n",
      "|6  |spark hadoop spark|[0.22922657813243238,0.7707734218675676] |1.0       |\n",
      "|7  |apache hadoop     |[0.9768636139518374,0.023136386048162642]|0.0       |\n",
      "+---+------------------+-----------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val test = spark.createDataFrame(Seq(\n",
    "  (4L, \"spark i j k\"),\n",
    "  (5L, \"l m n\"),\n",
    "  (6L, \"spark hadoop spark\"),\n",
    "  (7L, \"apache hadoop\")\n",
    ")).toDF(\"Id\", \"Text\")\n",
    "\n",
    "pipelineModel.transform(test).select(\"id\", \"Text\", \"probability\", \"prediction\")show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The ``DataFrame`` we provided just contains the text. The pipeline internally tokenizes and then computes the feature vector before feeding it into the classifier.\n",
    "\n",
    "The process of training the model is not cheap and takes hours and at times days. We thus need a way to save the trained ``PipelineModel`` and even the ``Pipeline`` (in this case we need to train the model all over again). Following code snippet shows how we can achieve this.  We will load the pre-trained model and get predictions for our test ``DataFrame``. The predictions should be identical to the ones we just saw.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------------------+----------+\n",
      "|id |Text              |probability                              |prediction|\n",
      "+---+------------------+-----------------------------------------+----------+\n",
      "|4  |spark i j k       |[0.5406433544852275,0.45935664551477245] |0.0       |\n",
      "|5  |l m n             |[0.933438262738352,0.06656173726164803]  |0.0       |\n",
      "|6  |spark hadoop spark|[0.22922657813243238,0.7707734218675676] |1.0       |\n",
      "|7  |apache hadoop     |[0.9768636139518374,0.023136386048162642]|0.0       |\n",
      "+---+------------------+-----------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel.write.overwrite.save(\"pipeline-model\")\n",
    "pipe.write.overwrite.save(\"pipeline\")\n",
    "\n",
    "val preTrained = PipelineModel.load(\"pipeline-model\")\n",
    "preTrained.transform(test).select(\"id\", \"Text\", \"probability\", \"prediction\")show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Feature Extraction, Selection and Transformation.\n",
    "\n",
    "##### TF-IDF.\n",
    "\n",
    "This is a feature vector used in Natural Language Processing to analyse the importance of a term in the document. The TermFrequency ``TF(t, d)`` is used to denote the number of times the term occured in the document. The Document ``DF(t, D)`` is the number of times the term occured in the document corpus ``D``.\n",
    "\n",
    "$IDF(t, D)\\:= log\\frac{|D| + 1}{DF(t, D) + 1}$\n",
    "\n",
    "if the term is very common across corpus the fraction tends to 1 and the log becomes 0. If the word is rare in the corpus, the value of the fraction is large and thus the log value is large.\n",
    "\n",
    "Thus $TFIDF(t, d, D)\\:=\\:TF(t, d)\\cdot IDF(t, D)$\n",
    "\n",
    "We have already seen the HashingTF which calculates the term frequency in the document using Hashing. another alternative is ``CountVectorizer``. Following is the result of using both implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------------------------------------+\n",
      "|words             |rawFeatures                                          |\n",
      "+------------------+-----------------------------------------------------+\n",
      "|[a, b, c]         |(262144,[28698,30913,227410],[1.0,1.0,1.0])          |\n",
      "|[a, b, b, c, a, d]|(262144,[27526,28698,30913,227410],[1.0,1.0,2.0,2.0])|\n",
      "+------------------+-----------------------------------------------------+\n",
      "\n",
      "+------------------+-------------------------------+\n",
      "|words             |rawFeatures                    |\n",
      "+------------------+-------------------------------+\n",
      "|[a, b, c]         |(4,[0,1,2],[1.0,1.0,1.0])      |\n",
      "|[a, b, b, c, a, d]|(4,[0,1,2,3],[2.0,2.0,1.0,1.0])|\n",
      "+------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{HashingTF, Tokenizer, CountVectorizer}\n",
    "val sentenceData = spark.createDataFrame(Seq(\n",
    "  (0.0, \"a b c\"),\n",
    "  (0.0, \"a b b c a d\")\n",
    ")).toDF(\"label\", \"sentence\")\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"sentence\").setOutputCol(\"words\")\n",
    "val wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "val hashingTF = new HashingTF().setInputCol(\"words\").setOutputCol(\"rawFeatures\")\n",
    "val countTF = new CountVectorizer().setInputCol(\"words\").setOutputCol(\"rawFeatures\").fit(wordsData)\n",
    "hashingTF.transform(wordsData).select(\"words\", \"rawFeatures\")show(truncate = false)\n",
    "val tfDF = countTF.transform(wordsData)\n",
    "tfDF.select(\"words\", \"rawFeatures\").show(truncate = false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see in the above the output of both ``HashingTF`` and ``CountVectorizer`` both give is a ``Tuple3`` with the fields (``numFields``, ``featureId``, ``wordCount``) \n",
    "\n",
    "The size of ``featureId`` and ``wordCount`` is same and has a 1-1 correspondence between the term id and the term count.\n",
    "\n",
    "Now that we know what the feature count is, we then find the Inverse document frequency. Note how the term ``d`` gets a higher value and others get a value of 0 as they appear in all the documents in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------+----------------------------------------------+\n",
      "|words             |rawFeatures                    |tfidf                                         |\n",
      "+------------------+-------------------------------+----------------------------------------------+\n",
      "|[a, b, c]         |(4,[0,1,2],[1.0,1.0,1.0])      |(4,[0,1,2],[0.0,0.0,0.0])                     |\n",
      "|[a, b, b, c, a, d]|(4,[0,1,2,3],[2.0,2.0,1.0,1.0])|(4,[0,1,2,3],[0.0,0.0,0.0,0.4054651081081644])|\n",
      "+------------------+-------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.IDF\n",
    "\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"tfidf\")\n",
    "val idfModel = idf.fit(tfDF)\n",
    "idfModel.transform(tfDF).select(\"words\", \"rawFeatures\", \"tfidf\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Word2Vec\n",
    "\n",
    "In Word2Vec we represent words as word vectors such that similar words are close to each other in vector space.\n",
    "\n",
    "In Word2Vec we have two vectors for each word w, $u_w$ and $v_w$ when the word w is the context word and the center word. There is another parameter k which is for the context window size.\n",
    "\n",
    "Word2Vec has two possible approaches to calculate the word vectors, Skip gram and CBOW(Continuous Bag of Words). Spark use Skip gram approach to calculate the word vectors. The cost function that is maximized for training this model is\n",
    "\n",
    "$\\frac{1}{T}\\sum_{t = 1}^T\\sum_{j = -k}^klog(\\frac{p_{t+j}}{p_t})$\n",
    "\n",
    "Since log of a fraction is negative, we maximize as we need the number to be close to 0.\n",
    "\n",
    "Thus, given the center word $w_j$, the probability that the word $w_i$ is in the window is \n",
    "\n",
    "$p(w_i / w_j)\\:=\\:\\frac{exp(u_{w_i}^T\\cdot v_{w_j})}{\\sum_{l = 1}^V exp(u_l^T\\cdot v_{w_j})}$\n",
    "\n",
    "Following code snippet, calculates the word vectors for the vocabulary. The Vector we get for each sentence is a mean of all the vector for the words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------------------------------------------------------------+\n",
      "|text                                      |vector                                                         |\n",
      "+------------------------------------------+---------------------------------------------------------------+\n",
      "|[Hi, I, heard, about, Spark]              |[0.03173386193811894,0.009443491697311401,0.024377789348363876]|\n",
      "|[I, wish, Java, could, use, case, classes]|[0.025682436302304268,0.0314303718706859,-0.01815584538105343] |\n",
      "|[Logistic, regression, models, are, neat] |[0.022586782276630402,-0.01601201295852661,0.05122732147574425]|\n",
      "+------------------------------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "\n",
    "val documentDF = spark.createDataFrame(Seq(\n",
    "  \"Hi I heard about Spark\".split(\" \"),\n",
    "  \"I wish Java could use case classes\".split(\" \"),\n",
    "  \"Logistic regression models are neat\".split(\" \")\n",
    ").map(Tuple1.apply)).toDF(\"text\")\n",
    "\n",
    "val w2v = new Word2Vec().setInputCol(\"text\").setOutputCol(\"vector\").setVectorSize(3).setMinCount(0)\n",
    "val w2vModel = w2v.fit(documentDF)\n",
    "val vectors = w2vModel.transform(documentDF)\n",
    "vectors.show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### Feature Transformers\n",
    "\n",
    "##### Tokenizer\n",
    "\n",
    "We split a sentence into word in this type of transformation. By default the splitting is done by spaces. Following code snippet demonstrated this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------+------------------------------------------+\n",
      "|Id |Text                               |tokens                                    |\n",
      "+---+-----------------------------------+------------------------------------------+\n",
      "|1  |Hi I heard about Spark             |[hi, i, heard, about, spark]              |\n",
      "|2  |I wish Java could use case classes |[i, wish, java, could, use, case, classes]|\n",
      "|3  |Logistic regression models are neat|[logistic, regression, models, are, neat] |\n",
      "+---+-----------------------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val testDF = spark.createDataFrame(Seq(\n",
    "            (1, \"Hi I heard about Spark\"),\n",
    "            (2, \"I wish Java could use case classes\"),\n",
    "            (3, \"Logistic regression models are neat\")\n",
    "            )).toDF(\"Id\", \"Text\")\n",
    "val tokenizer = new Tokenizer().setInputCol(\"Text\").setOutputCol(\"tokens\")\n",
    "val tokenized = tokenizer.transform(testDF)\n",
    "tokenized.show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StopWordRemover \n",
    "\n",
    "Takes in tokenized dataset and adds new column which contains the subset of tokenized words except for stop words. A List of stop words for different languages can be found [here](https://github.com/apache/spark/tree/master/mllib/src/main/resources/org/apache/spark/ml/feature/stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------------------------------+\n",
      "|tokens                                    |s_tokens                            |\n",
      "+------------------------------------------+------------------------------------+\n",
      "|[hi, i, heard, about, spark]              |[hi, heard, spark]                  |\n",
      "|[i, wish, java, could, use, case, classes]|[wish, java, use, case, classes]    |\n",
      "|[logistic, regression, models, are, neat] |[logistic, regression, models, neat]|\n",
      "+------------------------------------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val stopWordTokenizer = new StopWordsRemover().setInputCol(\"tokens\").setOutputCol(\"s_tokens\")\n",
    "val stopWordDF = stopWordTokenizer.transform(tokenized)\n",
    "stopWordDF.select(\"tokens\", \"s_tokens\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### NGrams\n",
    "\n",
    "The following transformer create n grams from tokenized words, by default the value of n is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------------------------------------------------+\n",
      "|s_tokens                            |nGrams                                               |\n",
      "+------------------------------------+-----------------------------------------------------+\n",
      "|[hi, heard, spark]                  |[hi heard, heard spark]                              |\n",
      "|[wish, java, use, case, classes]    |[wish java, java use, use case, case classes]        |\n",
      "|[logistic, regression, models, neat]|[logistic regression, regression models, models neat]|\n",
      "+------------------------------------+-----------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.NGram\n",
    "\n",
    "val nGramsTransformer = new NGram().setInputCol(\"s_tokens\").setOutputCol(\"nGrams\")\n",
    "val nGramsDF = nGramsTransformer.transform(stopWordDF)\n",
    "nGramsDF.select(\"s_tokens\", \"nGrams\")show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Binarizer\n",
    "\n",
    "This transformer takes in a ``DataFrame`` and emits a scalar/vector of same size with 1 if the value is greater than the threshold else 0.\n",
    "The value in the output column is either a Dense or sparse vector whichever takes less space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|Vectors              |Binaries             |\n",
      "+---------------------+---------------------+\n",
      "|[1.0,2.0,3.0,4.0,5.0]|[0.0,0.0,1.0,1.0,1.0]|\n",
      "|[2.0,1.0,0.0,1.0,5.0]|(5,[4],[1.0])        |\n",
      "+---------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Binarizer\n",
    "val numericDF = spark.createDataFrame(Seq((1, Vectors.dense(1, 2, 3, 4, 5)), \n",
    "                    (1, Vectors.dense(2, 1, 0, 1, 5)))).toDF(\"Id\", \"Vectors\")\n",
    "val binarizer = new Binarizer().setInputCol(\"Vectors\").setOutputCol(\"Binaries\").setThreshold(2)\n",
    "binarizer.transform(numericDF).select(\"Vectors\", \"Binaries\").show(truncate = false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### PCA\n",
    "\n",
    "This Model can be used to perfome [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) for the given input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------------------------------------------+\n",
      "|features             |pcaFeatures                                                |\n",
      "+---------------------+-----------------------------------------------------------+\n",
      "|(5,[1,3],[1.0,7.0])  |[1.6485728230883807,-4.013282700516296,-5.524543751369388] |\n",
      "|[2.0,0.0,3.0,4.0,5.0]|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|\n",
      "|[4.0,0.0,0.0,6.0,7.0]|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |\n",
      "+---------------------+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.PCA\n",
    "val data = Array(\n",
    "  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),\n",
    "  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n",
    "  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0)\n",
    ")\n",
    "val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "val pca = new PCA().setInputCol(\"features\").setOutputCol(\"pcaFeatures\").setK(3).fit(df)\n",
    "\n",
    "val result = pca.transform(df)\n",
    "result.show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Polynomial Expansion\n",
    "\n",
    "It is a process of expanding the features into polynomial space. For example, with input vector [2, 1] and degree of 3 we have the following possible values $[2^1, 2^2, 2^3, 1^1, 1^2, 1^3, 1\\cdot2, 1^2\\cdot 2, 1\\cdot2^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------------------------------+\n",
      "|features  |polyFeatures                              |\n",
      "+----------+------------------------------------------+\n",
      "|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |\n",
      "|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |\n",
      "|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|\n",
      "+----------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.PolynomialExpansion\n",
    "\n",
    "val data = Array(\n",
    "  Vectors.dense(2.0, 1.0),\n",
    "  Vectors.dense(0.0, 0.0),\n",
    "  Vectors.dense(3.0, -1.0)\n",
    ")\n",
    "val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "val polyExpansion = new PolynomialExpansion().setInputCol(\"features\").setOutputCol(\"polyFeatures\").setDegree(3)\n",
    "\n",
    "val polyDF = polyExpansion.transform(df)\n",
    "polyDF.show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### StringIndexer and OneHotEncoder\n",
    "\n",
    "These transformers transform a string value to an index and the index column to a vector with maximum one bit set corresponding to the index value.\n",
    "\n",
    "The one hot encoding in thie case doesn't have 3 bits for three distinct values but achieves this using 2 bits only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+---------+\n",
      "| id|category|categoryIndex|   OneHot|\n",
      "+---+--------+-------------+---------+\n",
      "|  0|       a|          0.0|[1.0,0.0]|\n",
      "|  1|       b|          2.0|[0.0,0.0]|\n",
      "|  2|       c|          1.0|[0.0,1.0]|\n",
      "|  3|       a|          0.0|[1.0,0.0]|\n",
      "|  4|       a|          0.0|[1.0,0.0]|\n",
      "|  5|       c|          1.0|[0.0,1.0]|\n",
      "+---+--------+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (0, \"a\"),\n",
    "  (1, \"b\"),\n",
    "  (2, \"c\"),\n",
    "  (3, \"a\"),\n",
    "  (4, \"a\"),\n",
    "  (5, \"c\")\n",
    "  )).toDF(\"id\", \"category\")\n",
    "\n",
    "val indexer = new StringIndexer().setInputCol(\"category\").setOutputCol(\"categoryIndex\").fit(df)\n",
    "val indexed = indexer.transform(df)\n",
    "\n",
    "val encoder = new OneHotEncoder().setInputCol(\"categoryIndex\").setOutputCol(\"categoryVec\")\n",
    "\n",
    "val encoded = encoder.transform(indexed)\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val toDense = udf((x: Vector) => x.toDense)\n",
    "encoded.withColumn(\"OneHot\", toDense($\"categoryVec\")).select(\"id\",\"category\", \"categoryIndex\", \"OneHot\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VectorIndexer \n",
    "Converts the dimensions of the vectors to categorical fields wherever the number of unique values in the dimension of less than or equal to the max categories available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the category map is Set(0)\n",
      "+---+----------+-----------+\n",
      "| id|   vectors|Categorized|\n",
      "+---+----------+-----------+\n",
      "|  0|[10.0,1.0]|  [1.0,1.0]|\n",
      "|  2| [9.0,2.0]|  [0.0,2.0]|\n",
      "|  3|[10.0,3.0]|  [1.0,3.0]|\n",
      "|  4| [9.0,4.0]|  [0.0,4.0]|\n",
      "|  5|[10.0,5.0]|  [1.0,5.0]|\n",
      "+---+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "\n",
    "val df = spark.createDataFrame(\n",
    "                Seq((0, Vectors.dense(10, 1)),\n",
    "                (2, Vectors.dense(9, 2)),\n",
    "                (3, Vectors.dense(10, 3)),\n",
    "                (4, Vectors.dense(9, 4)),\n",
    "                (5, Vectors.dense(10, 5)))).toDF(\"id\", \"vectors\")\n",
    "  \n",
    "val vi = new VectorIndexer().setInputCol(\"vectors\").setMaxCategories(2).setOutputCol(\"Categorized\")\n",
    "val viModel = vi.fit(df)\n",
    "println(\"The index of the category map is \" + viModel.categoryMaps.keys)\n",
    "viModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Interaction and VectorAssembler\n",
    "\n",
    "VectorAssembler takes in multiple vectors and scalers to create one vector in the output.  Interaction simply takes in multiple vectors to create another vector of with values which is a cross product of all possible values of these vectors. Following is an example code snippet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+----------------------------------------------+\n",
      "|vec1          |vec2          |interactionCol                                |\n",
      "+--------------+--------------+----------------------------------------------+\n",
      "|[1.0,2.0,3.0] |[8.0,4.0,5.0] |[8.0,4.0,5.0,16.0,8.0,10.0,24.0,12.0,15.0]    |\n",
      "|[4.0,3.0,8.0] |[7.0,9.0,8.0] |[28.0,36.0,32.0,21.0,27.0,24.0,56.0,72.0,64.0]|\n",
      "|[6.0,1.0,9.0] |[2.0,3.0,6.0] |[12.0,18.0,36.0,2.0,3.0,6.0,18.0,27.0,54.0]   |\n",
      "|[10.0,8.0,6.0]|[9.0,4.0,5.0] |[90.0,40.0,50.0,72.0,32.0,40.0,54.0,24.0,30.0]|\n",
      "|[9.0,2.0,7.0] |[10.0,7.0,3.0]|[90.0,63.0,27.0,20.0,14.0,6.0,70.0,49.0,21.0] |\n",
      "|[1.0,1.0,4.0] |[2.0,8.0,4.0] |[2.0,8.0,4.0,2.0,8.0,4.0,8.0,32.0,16.0]       |\n",
      "+--------------+--------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.Interaction\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (1, 1, 2, 3, 8, 4, 5),\n",
    "  (2, 4, 3, 8, 7, 9, 8),\n",
    "  (3, 6, 1, 9, 2, 3, 6),\n",
    "  (4, 10, 8, 6, 9, 4, 5),\n",
    "  (5, 9, 2, 7, 10, 7, 3),\n",
    "  (6, 1, 1, 4, 2, 8, 4)\n",
    ")).toDF(\"id1\", \"id2\", \"id3\", \"id4\", \"id5\", \"id6\", \"id7\")\n",
    "\n",
    "val assembler1 = new VectorAssembler().setInputCols(Array(\"id2\", \"id3\", \"id4\")).setOutputCol(\"vec1\")\n",
    "val assembler2 = new VectorAssembler().setInputCols(Array(\"id5\", \"id6\", \"id7\")).setOutputCol(\"vec2\")\n",
    "\n",
    "val df1 = assembler2.transform(assembler1.transform(df))\n",
    "\n",
    "\n",
    "val interaction = new Interaction().setInputCols(Array(\"vec1\", \"vec2\")).setOutputCol(\"interactionCol\")\n",
    "val df2 = interaction.transform(df1)\n",
    "df2.select(\"vec1\", \"vec2\", \"interactionCol\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Normalizer\n",
    "\n",
    "Normalizes the vector by dividing each value in the vector with the normaizer. The normalizer is $\\sqrt[p]{\\sum_{i = 0}^N |v_i|^p}$ where N is the length of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------------------------------------------+\n",
      "|vec1          |NormalizedVec1                                              |\n",
      "+--------------+------------------------------------------------------------+\n",
      "|[1.0,2.0,3.0] |[0.2672612419124244,0.5345224838248488,0.8017837257372732]  |\n",
      "|[4.0,3.0,8.0] |[0.423999152002544,0.31799936400190804,0.847998304005088]   |\n",
      "|[6.0,1.0,9.0] |[0.552344770738994,0.09205746178983235,0.8285171561084911]  |\n",
      "|[10.0,8.0,6.0]|[0.7071067811865475,0.565685424949238,0.4242640687119285]   |\n",
      "|[9.0,2.0,7.0] |[0.7774815830232241,0.17277368511627203,0.6047078979069521] |\n",
      "|[1.0,1.0,4.0] |[0.23570226039551587,0.23570226039551587,0.9428090415820635]|\n",
      "+--------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val normalizer = new Normalizer().setInputCol(\"vec1\").setOutputCol(\"NormalizedVec1\").setP(2)\n",
    "normalizer.transform(df1).select(\"vec1\", \"NormalizedVec1\").show(truncate = false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### StandardScaler\n",
    "\n",
    "Standardizes each vector to have unit standard deviation and zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------------------------------------------+\n",
      "|Features     |StdFeatures                                              |\n",
      "+-------------+---------------------------------------------------------+\n",
      "|[5.0,2.0,7.0]|[1.7677669529663687,2.82842712474619,4.949747468305832]  |\n",
      "|[1.0,3.0,9.0]|[0.35355339059327373,4.242640687119285,6.363961030678928]|\n",
      "+-------------+---------------------------------------------------------+\n",
      "\n",
      "+-------------+---------------------------------------------------------+\n",
      "|Features     |StdFeatures                                              |\n",
      "+-------------+---------------------------------------------------------+\n",
      "|[5.0,2.0,7.0]|[1.7677669529663687,2.82842712474619,4.949747468305832]  |\n",
      "|[1.0,3.0,9.0]|[0.35355339059327373,4.242640687119285,6.363961030678928]|\n",
      "+-------------+---------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val simpleDF = spark.createDataFrame(Seq(\n",
    "                    Vectors.dense(5, 2, 7), \n",
    "                    Vectors.dense(1, 3, 9)).map(Tuple1.apply)).toDF(\"Features\")\n",
    "\n",
    "val ss1 = new StandardScaler().setInputCol(\"Features\").setOutputCol(\"StdFeatures\").setWithMean(false).setWithStd(true)\n",
    "val ssModel = ss.fit(simpleDF)\n",
    "ssModel.transform(simpleDF).show(truncate = false)\n",
    "val ss2 = new StandardScaler().setInputCol(\"Features\").setOutputCol(\"StdFeatures\").setWithMean(true).setWithStd(true)\n",
    "val ssModel1 = ss.fit(simpleDF)\n",
    "ssModel1.transform(simpleDF).show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation is done by calculating standard deviation across all dimensions of the vectors in different rows and calculating the z-scores. In one case the values are centered across mean and in another it isnt.\n",
    "\n",
    "\n",
    "##### MinMaxScaler\n",
    "\n",
    "MinMaxScaler scales each feature between a given range. The rescaling each feature E is done as follows\n",
    "\n",
    "$Rescaled(e_i)\\:=\\:\\frac{e_i - E}{E_{max} - E_{min}} * (max - min) + min$\n",
    "\n",
    "Default value of max and min is 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------------------+\n",
      "|Features     |StdFeatures                    |\n",
      "+-------------+-------------------------------+\n",
      "|[5.0,2.0,7.0]|[1.0,-1.0,-0.33333333333333337]|\n",
      "|[1.0,3.0,9.0]|[-1.0,-0.6666666666666667,1.0] |\n",
      "|[4.0,8.0,6.0]|[0.5,1.0,-1.0]                 |\n",
      "+-------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.MinMaxScaler\n",
    "\n",
    "val simpleDF = spark.createDataFrame(Seq(\n",
    "                    Vectors.dense(5, 2, 7), \n",
    "                    Vectors.dense(1, 3, 9),\n",
    "                    Vectors.dense(4, 8, 6)).map(Tuple1.apply)).toDF(\"Features\")\n",
    "\n",
    "val minMaxScaler = new MinMaxScaler().setInputCol(\"Features\").setOutputCol(\"StdFeatures\").setMin(-1).setMax(1)\n",
    "val minMaxScalerModel = minMaxScaler.fit(simpleDF)\n",
    "minMaxScalerModel.transform(simpleDF).show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Bucketizer\n",
    "\n",
    "Splits the feature column values specified by the user into buckets. With n + 1 splits, there are n splits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|features|bucketedFeatures|\n",
      "+--------+----------------+\n",
      "|  -999.9|             0.0|\n",
      "|    -0.5|             1.0|\n",
      "|    -0.3|             1.0|\n",
      "|     0.0|             2.0|\n",
      "|     0.2|             2.0|\n",
      "|   999.9|             3.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val splits = Array(Double.NegativeInfinity, -0.5, 0.0, 0.5, Double.PositiveInfinity)\n",
    "\n",
    "val data = Array(-999.9, -0.5, -0.3, 0.0, 0.2, 999.9)\n",
    "val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\n",
    "\n",
    "val bucketizer = new Bucketizer().setInputCol(\"features\").setOutputCol(\"bucketedFeatures\").setSplits(splits)\n",
    "\n",
    "val bucketedData = bucketizer.transform(dataFrame)\n",
    "\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### ElementwiseProduct\n",
    "\n",
    "This transformer performs element wise multiplication between the given transformer vector and each vector row in the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------------+\n",
      "| id|       vector|transformedVector|\n",
      "+---+-------------+-----------------+\n",
      "|  a|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|\n",
      "|  b|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|\n",
      "+---+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ElementwiseProduct\n",
    "\n",
    "\n",
    "val dataFrame = spark.createDataFrame(Seq(\n",
    "  (\"a\", Vectors.dense(1.0, 2.0, 3.0)),\n",
    "  (\"b\", Vectors.dense(4.0, 5.0, 6.0)))).toDF(\"id\", \"vector\")\n",
    "\n",
    "val transformingVector = Vectors.dense(0.0, 1.0, 2.0)\n",
    "val transformer = new ElementwiseProduct().setScalingVec(transformingVector).setInputCol(\"vector\").setOutputCol(\"transformedVector\")\n",
    "\n",
    "transformer.transform(dataFrame).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### SQLTransformer\n",
    "\n",
    "This transformer uses SQL queries to transform data frames to other dataframes. We can use built in functions provided by Spark SQL or UDFs to achieve this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+\n",
      "| id| v1| v2| v3|  v4|\n",
      "+---+---+---+---+----+\n",
      "|  0|1.0|3.0|4.0| 3.0|\n",
      "|  2|2.0|5.0|7.0|10.0|\n",
      "+---+---+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.SQLTransformer\n",
    "\n",
    "val df = spark.createDataFrame(\n",
    "  Seq((0, 1.0, 3.0), (2, 2.0, 5.0))).toDF(\"id\", \"v1\", \"v2\")\n",
    "\n",
    "val sqlTransformer = new SQLTransformer().setStatement(\"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__\")\n",
    "sqlTransformer.transform(df).show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### QuantileDiscretizer\n",
    "\n",
    "Transforms the given input continuous range of values into discrete quantiles.\n",
    "\n",
    "TODO: Analyse the code and algorithm.\n",
    "\n",
    "##### Imputer\n",
    "\n",
    "The imputer imputes missing values in a column to either mean or median value as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+-----+\n",
      "|  a|  b|out_a|out_b|\n",
      "+---+---+-----+-----+\n",
      "|1.0|NaN|  1.0|  4.0|\n",
      "|2.0|NaN|  2.0|  4.0|\n",
      "|NaN|3.0|  2.0|  3.0|\n",
      "|4.0|4.0|  4.0|  4.0|\n",
      "|5.0|5.0|  5.0|  5.0|\n",
      "+---+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Imputer\n",
    "\n",
    "val df = spark.createDataFrame(Seq(\n",
    "  (1.0, Double.NaN),\n",
    "  (2.0, Double.NaN),\n",
    "  (Double.NaN, 3.0),\n",
    "  (4.0, 4.0),\n",
    "  (5.0, 5.0)\n",
    ")).toDF(\"a\", \"b\")\n",
    "\n",
    "val imputer = new Imputer().setInputCols(Array(\"a\", \"b\")).setOutputCols(Array(\"out_a\", \"out_b\")).setStrategy(\"median\")\n",
    "val imputingModel = imputer.fit(df)\n",
    "imputingModel.transform(df).show\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO: Explore Local Sensitivity Hashing for Spark DF.\n",
    "\n",
    "\n",
    "### Classification and Regression\n",
    "\n",
    "This part of the notebook will focus on how to implement ML models in Spark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

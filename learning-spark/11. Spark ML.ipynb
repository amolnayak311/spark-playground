{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Spark ML\n",
    "\n",
    "This notebook will introduce Spark ML and its API.\n",
    "\n",
    "#### Correlation calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"SparkML\").getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "1.0                   0.055641488407465814  NaN  0.4004714203168137  \n",
      "0.055641488407465814  1.0                   NaN  0.9135958615342522  \n",
      "NaN                   NaN                   1.0  NaN                 \n",
      "0.4004714203168137    0.9135958615342522    NaN  1.0                 \n",
      "\n",
      "Spearman correlation matrix:\n",
      "1.0                  0.10540925533894532  NaN  0.40000000000000174  \n",
      "0.10540925533894532  1.0                  NaN  0.9486832980505141   \n",
      "NaN                  NaN                  1.0  NaN                  \n",
      "0.40000000000000174  0.9486832980505141   NaN  1.0                  \n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.ml.linalg.{Vectors, Matrix}\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "  Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),  \n",
    "  Vectors.dense(4.0, 5.0, 0.0, 3.0),\n",
    "  Vectors.dense(6.0, 7.0, 0.0, 8.0),\n",
    "  Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))\n",
    ")\n",
    "\n",
    "val df = data.map(Tuple1.apply).toDF(\"features\")\n",
    "val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\n",
    "println(\"Pearson correlation matrix:\\n\" + coeff1.toString)\n",
    "val Row(coeff2: Matrix) = Correlation.corr(df, \"features\", \"spearman\").head\n",
    "println(\"\\nSpearman correlation matrix:\\n\" + coeff2.toString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let us understand the above piece of code step by step. There are two ways to create vectors, first one is dense and another is sparse. In a dense vector we specify all n elements and its values. The dense vector is simply created as ``Vectors.dense(4.0, 5.0, 0.0, 3.0)`` where as sparse is created as ``Vectors.sparse(4, Seq((0, 1.0), (3, -2.0)))`` wgere the first number if the number of elements/dimension in the vector, and the ``Seq`` given is a tuple of index, (value pair). Thus ``Vectors.sparse(4, Seq((0, 1.0), (3, -2.0)))`` is same as ``Vectors.dense(1.0, 0.0, 0.0, -2.0)`` as seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,0.0,0.0,-2.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))).toDense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The correlation matrix is is always symmetric across diagonal with diagonal values being 1 as the corelation of a vector with itself is always 1. The matrix is symmetric across the diagonal that is element (0, 1) is same as (1, 0), (0, 2) same as (2, 0) and so on.\n",
    "\n",
    "The formula for pearson correlation is \n",
    "\n",
    "$p\\:=\\:\\frac{n\\sum{xy} - (\\sum{x})(\\sum{y})}{\\sqrt{[n\\sum{x^2} - (\\sum{x})^2][n\\sum{y^2} - (\\sum{y})^2]}}$\n",
    "\n",
    "The correlation between two lists (1.0, 4.0, 6.0, 9.0) and (0.0, 5.0, 7.0, 0.0)\n",
    "as per [this](http://calculator.vhex.net/calculator/statistics/pearson-correlation) URL is expected to be 0.055641.\n",
    "\n",
    "The following code snippet calculates this pearson correlation between two list of doubles of equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson coefficient between v1 and v2 is 0.055641488407465724\n"
     ]
    }
   ],
   "source": [
    "import scala.math.sqrt\n",
    "\n",
    "def pearsonCorrelation(x:List[Double], y: List[Double]): Double = {\n",
    "    val n = x.length\n",
    "    val sumx = x.reduce(_ + _)\n",
    "    val sumxsquare = x.map(e => e * e).reduce(_ + _)\n",
    "    val sumy = y.reduce(_ + _)\n",
    "    val sumysquare = y.map(e => e * e).reduce(_ + _)\n",
    "    val sumxy = (x zip y).map{case (l, r) => l * r}.reduce(_ + _)\n",
    "    val numerator = (n * sumxy) - (sumx * sumy)\n",
    "    val denominator = (n * sumxsquare - sumx * sumx) * (n * sumysquare - sumy * sumy)\n",
    "    numerator / math.sqrt(denominator)\n",
    "}\n",
    "\n",
    "val v1 = List(1.0, 4.0, 6.0, 9.0)\n",
    "val v2 = List(0, 5.0, 7.0, 0)\n",
    "println(\"Pearson coefficient between v1 and v2 is \" + pearsonCorrelation(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance between x and y is 0.055641488407465724\n"
     ]
    }
   ],
   "source": [
    "// Alternate implementation of above but by calculating the mean value first. \n",
    "// The above implementation doesn't need to calculate\n",
    "val n = v1.length\n",
    "val meanx = v1.reduce(_ + _) * 1.0 / n\n",
    "val meany = v2.reduce(_ + _) * 1.0 / n\n",
    "\n",
    "val numerator = (v1 zip v2).map{\n",
    " case (e1, e2) => (e1 - meanx) * (e2 - meany)\n",
    "}.reduce(_ + _)\n",
    "\n",
    "val sumxsquare = v1.map( e => (e - meanx) * (e - meanx)).reduce(_ + _)\n",
    "val sumysquare= v2.map(e => (e - meany) * (e - meany)).reduce(_ + _)\n",
    "val denominator = math.sqrt(sumxsquare) * math.sqrt(sumysquare)\n",
    "\n",
    "\n",
    "println(\"Covariance between x and y is \" + numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The vectors defined in ``data`` for which we computed the pearson coefficient are\n",
    "\n",
    "```\n",
    "[1.0,0.0,0.0,-2.0]\n",
    "[4.0,5.0,0.0,3.0]\n",
    "[6.0,7.0,0.0,8.0]\n",
    "[9.0,0.0,0.0,1.0]\n",
    "```\n",
    "\n",
    "The line ``val df = data.map(Tuple1.apply).toDF(\"features\")`` creates a ``DataFrame``. The ``toDF(columnName)`` is a a function that can be applied to ``Seq[Tuple{n}]``. This is an implicit function that lets us convert tuples to data frame which we get by the import ``scala.implicits._``. Each tuple in the sequence becomes a row and each element of the tuple becomes a column. To achieve this, we need to convert ``Seq[Vector]`` to ``Seq[Tuple1]`` using ``map`` so that we can convert the ``Seq[Tuple1]`` to a ``DataFrame`` with one column which we will call features.\n",
    "\n",
    "The code ``Correlation.corr(df, \"features\")`` computes the correlation matrix. The return value of this call is a ``DataFrame`` with one row and one column. The name of the column is ``pearson(<nae of the original column in dataset>)`` and the type of the value is a ``Matrix``. The code ``val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head`` is a one liner to get the first and only row of this ``DataFrame`` and assign the value of the ``Matrix`` in it to the variable ``coeff1``.\n",
    "\n",
    "The parameters of the ``corr`` function are the ``DataFrame`` instance, the name of the column in the ``DataFrame`` and an optional third parameter for the correlation method, which defaults to ``pearson``. Only valid value for now is ``spearman``.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### ChiSquared Test\n",
    "\n",
    "We will now look at Hypothesis testing which tests whether the result we get is stastically significant or not. We will see Pearsons Chi-Squared tests in this section. First a code sample and the result we get\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the ChiSquaredTest DataFrame\n",
      "+---------------------------------------+----------------+----------+\n",
      "|pValues                                |degreesOfFreedom|statistics|\n",
      "+---------------------------------------+----------------+----------+\n",
      "|[0.6872892787909721,0.6822703303362126]|[2, 3]          |[0.75,1.5]|\n",
      "+---------------------------------------+----------------+----------+\n",
      "\n",
      "pValues = [0.6872892787909721,0.6822703303362126]\n",
      "degreesOfFreedom = [2,3]\n",
      "statistics = [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "\n",
    "val data = Seq(\n",
    "  (0.0, Vectors.dense(0.5, 10.0)),\n",
    "  (0.0, Vectors.dense(1.5, 20.0)),\n",
    "  (1.0, Vectors.dense(1.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 30.0)),\n",
    "  (0.0, Vectors.dense(3.5, 40.0)),\n",
    "  (1.0, Vectors.dense(3.5, 40.0))\n",
    ")\n",
    "\n",
    "val df = data.toDF(\"label\", \"features\")\n",
    "val chiDF = ChiSquareTest.test(df, \"features\", \"label\")\n",
    "val chi = chiDF.head\n",
    "println(\"Printing the ChiSquaredTest DataFrame\")\n",
    "chiDF.show(truncate = false)\n",
    "println(\"pValues = \" + chi.getAs[Vector](0))\n",
    "println(\"degreesOfFreedom = \" + chi.getSeq[Int](1).mkString(\"[\", \",\", \"]\"))\n",
    "println(\"statistics = \" + chi.getAs[Vector](2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will now derive the above statistics using plain RDDs to see how the calculation is performed step by step. Note that this is not necessarily the most efficient way, but it demonstrates the steps nevertheless. \n",
    "\n",
    "We start by creating a contingency matrix for the each feature. The number of rows of this matrix are same as the number of unique values of that feature and the number of columns is same as number of unique labels. The values in the matrix are same as the number of occurances for that feature, label combination. In our case we will build two contingency matrix for each feature. For the first feature the unique values are (0.5, 1.5, 3.5) and the second feature has the unique values (10, 20, 30, 40). The number of columns are 2 in both cases for the labels (1, 0).\n",
    "\n",
    "As an illustration, the matrix for first feature would be as follows. We have also added the row sum and the col sum for the following matrix\n",
    "\n",
    "|               | **0**          | **1**  | **sum**|\n",
    "|:-------------: |:-------------:|:-------------: |:-------------: |\n",
    "| **0.5**      |  1| 0 |1|\n",
    "| **1.5**      | 1      |   1 |2|\n",
    "| **3.5**      | 2     |    1 |3|\n",
    "| **sum**      | 4    |   2  ||\n",
    "\n",
    "\n",
    "Let us give the input as a List of ``Tuples[(Double, Double)]``. This is for one dimension of the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Label, Feature) pairs counts are [(1.5,0) -> 1, (3.5,0) -> 2, (3.5,1) -> 1, (0.5,0) -> 1, (1.5,1) -> 1]\n",
      "Row Sum are [3.5 -> 3, 1.5 -> 2, 0.5 -> 1]\n",
      "Col Sum are [1 -> 2, 0 -> 4]\n"
     ]
    }
   ],
   "source": [
    "val inputList = List((0.5, 0), (1.5, 0), (1.5, 1), (3.5, 0), (3.5, 0), (3.5, 1))\n",
    "val inputs = sc.parallelize(inputList)\n",
    "val freq = inputs.map(i => (i, 1)).reduceByKey(_ + _).collectAsMap\n",
    "val rowSum = inputs.map(i => (i._1, 1)).reduceByKey(_ + _).collectAsMap\n",
    "val colSum = inputs.map(i => (i._2, 1)).reduceByKey(_ + _).collectAsMap\n",
    "val inputSize = inputList.size\n",
    "val uniqueLabels = inputList.map(_._2).toSet\n",
    "val uniqueFeatures = inputList.map(_._1).toSet\n",
    "println(\"(Label, Feature) pairs counts are \" + \n",
    "                freq.mkString(\"[\", \", \", \"]\") + \n",
    "                \"\\nRow Sum are \" + rowSum.mkString(\"[\", \", \", \"]\") + \n",
    "                \"\\nCol Sum are \" + colSum.mkString(\"[\", \", \", \"]\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above Map, Row Sum and Col Sum values are same as the the Matrix we saw earlier. The next step is to compute the expected value. The way we compute the expected value is compute the probability of each of the possible labels and then multiply the total number of occurances of the feature with these probability.\n",
    "\n",
    "For example, in the above case, from Col Sum we have the frequency of label 0 and 1 is 0.67 (4 / 6) and 0.33 (2 / 6) respectively. Expected value for feature value 3.5 would be 2( 3 $\\times$ 0.67) and 1( 3 $\\times$ 0.33) for label 0 and 1 respectively\n",
    "\n",
    "Following code computes the expected value of each feature value, label value combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected values for possible (Label, Feature) pair are Set(((1.5,0),1.3333333333333333), ((3.5,0),2.0), ((0.5,1),0.3333333333333333), ((0.5,0),0.6666666666666666), ((1.5,1),0.6666666666666666), ((3.5,1),1.0))\n"
     ]
    }
   ],
   "source": [
    "val expectedValues = (for(f <- uniqueFeatures ; l <-  uniqueLabels) yield(f, l)) map {\n",
    "    case (f, l) => ((f, l), 1.0 * rowSum(f) * colSum(l) / inputSize)\n",
    "}\n",
    "println(\"Expected values for possible (Label, Feature) pair are \" + expectedValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For calculating $\\chi^2$ stat. We will implement the following\n",
    "\n",
    "$\\chi^2\\:=\\:\\sum_{i=1}^{N}\\frac{(O_i - E_i)^2}{E_i}$\n",
    "\n",
    "Where\n",
    "\n",
    "- $O_i$ = the number of observations of type i.\n",
    "- N = Total Number of observations\n",
    "- $E_i$ = Expected theoritical probability of type i\n",
    "\n",
    "\n",
    "Degrees of freedom is simply computed as $(n_f - 1) \\times (n_l - 1)$\n",
    "\n",
    "Where \n",
    "\n",
    "- $n_f$: Number of unique feature values\n",
    "- $n_l$: Number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stat is 0.7500000000000001, degrees of freedom are 2\n"
     ]
    }
   ],
   "source": [
    "val stat = expectedValues.foldLeft(0.0){\n",
    "    case(acc, (k, e)) =>\n",
    "        val f = freq.getOrElse(k, 0)\n",
    "        val diff = f - e\n",
    "        acc + diff * diff / e        \n",
    "}\n",
    "val df = (uniqueFeatures.size - 1) * (uniqueLabels.size - 1)\n",
    "println(\"Stat is \" + stat + \", degrees of freedom are \" + df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "With the Stat and degrees of freedom calculated, we will now calculate the pValue. The above degrees of freedom and stat are in sync with what we calculated using SparkML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

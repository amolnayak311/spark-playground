{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Spark Programming\n",
    "\n",
    "We will revisit accumulators and broadcast variables in this notebook.\n",
    "\n",
    "---\n",
    "\n",
    "#### Accumumators\n",
    "\n",
    "When variable from the driver program are sent to functions used in say map or filter operations, their value is sent to these partitions and they can be used on different partitions, however, their final values are not propagated back to the driver program. Thus normal variables are ok to be used in the use case where we want to add a fixed number to all numbers in the RDD, but not in case where we want to say count some numbers across various partitions of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding delta to all values gives us List(3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n",
      "Value of count is 0"
     ]
    }
   ],
   "source": [
    "//Ok in the following case\n",
    "val delta = 2\n",
    "\n",
    "val rdd = sc.parallelize(1 to 10)\n",
    "println(\"Adding delta to all values gives us \" + rdd.map(_ + delta).collect.toList)\n",
    "\n",
    "var count = 0\n",
    "//Not ok for following case, where we want to count all even numbers\n",
    "\n",
    "rdd.foreach(x => if(x % 2 == 0) count += 1)\n",
    "\n",
    "print(\"Value of count is \" + count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Accumulators to the rescue. \n",
    "For the above use case we need to use an accumulator which is distributed across the cluster and does exactly what we need. Accumulator is similar to seeing one value across the entire cluster and the final result propagated back to driver program. Following code demonstrates the use case of counting even numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of countAcc is 5"
     ]
    }
   ],
   "source": [
    "val countAcc = sc.accumulator(0)\n",
    "rdd.foreach(x => if(x % 2 == 0) countAcc += 1)\n",
    "print(\"Value of countAcc is \" + countAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that accumulator value will be visible only when an action is executed. For example, the following mapping function males use of an accumulator but the value is not visible as transformations are lazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of countAcc1 is 0\n",
      "After executing an action on mappedRdd countAcc1 is 5\n",
      "After executing an action another time on mappedRdd countAcc1 is 10\n",
      "After invoking an action on cached RDD countAcc1 is 15\n",
      "After invoking an action another time on cached RDD countAcc1 is 15\n"
     ]
    }
   ],
   "source": [
    "val countAcc1 = sc.accumulator(0)\n",
    "val mappedRdd = rdd.map(x => {\n",
    "   if(x % 2 == 0) {\n",
    "      countAcc1 += 1\n",
    "      x\n",
    "   } else {\n",
    "       -x\n",
    "   }\n",
    "})\n",
    "println(\"Value of countAcc1 is \" + countAcc1)\n",
    "mappedRdd.count()\n",
    "println(\"After executing an action on mappedRdd countAcc1 is \" + countAcc1)\n",
    "mappedRdd.count()\n",
    "println(\"After executing an action another time on mappedRdd countAcc1 is \" + countAcc1)\n",
    "val cachedRdd = mappedRdd.cache()\n",
    "cachedRdd.count()\n",
    "println(\"After invoking an action on cached RDD countAcc1 is \" + countAcc1)\n",
    "cachedRdd.count()\n",
    "println(\"After invoking an action another time on cached RDD countAcc1 is \" + countAcc1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "An interesting observation of the above output is that each time an action is invoked on an RDD that's not cached, the mapping is re executed. So unless the RDD is cached in which case the mapping is invoked at least once and subsequent calls on any action of a cached RDD will not necessarily invoke the transformation and thus we will not consistently see the ``countAcc1`` incremented.\n",
    "\n",
    "This is an important observation and also a good reason why accumulators are not really a good idea to code a mission critical business logic and should be used for debugging or non mission critical logic only. For a use case like to count the even numbers (or a similar logic), we are better off using ``reduce`` or an ``aggregate`` function on RDD amongst many possible ways.\n",
    "\n",
    "Note that accumulators are available to be read only in driver program and not available to be read in the function running in parallel across the cluster. Simily allowing the workers to only write to the accumulator makes it easy to ensure integrity of the accumulator and not worry about propagating the state of the variable across the cluster. Following example demonstrates that the mapping function cannot read the value of the accumulator variable but just write to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 52, localhost, executor driver): java.lang.UnsupportedOperationException: Can't read accumulator value in task\n",
      "\tat org.apache.spark.Accumulable.value(Accumulable.scala:117)\n",
      "\tat $line176.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply$mcII$sp(<console>:28)\n",
      "\tat $line176.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:23)\n",
      "\tat $line176.$read$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:23)\n",
      "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
      "\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1835)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1162)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n",
      "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val acc = sc.accumulator(0)\n",
    "val mappedRdd1 = rdd.map(x => {\n",
    "   if(x % 2 == 0) {\n",
    "      countAcc1 += 1\n",
    "      x\n",
    "   } else {\n",
    "      x + countAcc1.value\n",
    "   }\n",
    "})\n",
    "try {\n",
    "    mappedRdd1.count()    \n",
    "} catch {\n",
    "    case e: org.apache.spark.SparkException => println(e.getMessage)\n",
    "    case e: Throwable => throw e\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Accumulators and Fault tolerance.\n",
    "\n",
    "We have already seen how the accumulator values are not reliable when used in a transformation. On each call of an action the transformation is triggered giving us an unreliable value. The value is not reliable even if the action was called only once. Its possible for large RDDs a partition might have failed or runs the operation very slowly triggering the execution of the transformation on the same chunk of data on another machine. This is how spark guarantees availibility and recovery from failure by executing the series of transformations failed chunks of data on alternate machines. When such duplicate execution of transformation occurs on chunks of data the value of the accumulator is not reliable.\n",
    "\n",
    "To reliably use the value of the accumulator variable, it should be used in the action like ``foreach`` and not in transformation.\n",
    "\n",
    "We can define custom Accumulators (TODO: Show with an example) provided that the operation we intend to implement is commutative and associative.\n",
    "\n",
    "\n",
    "### Broadcast variables\n",
    "\n",
    "Broadcast variables are read only variables those are available on each worker node. We will see a couple of examples and see where exactly can these be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(3, 1, 7, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "val rnd = scala.util.Random\n",
    "val someNumbers = (for (i <- 1 to 5) yield rnd.nextInt(10)).toList\n",
    "println(someNumbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We generated 5 numbers randomly and we now have a couple of filter operations which will give us a list of numbers excluding the above numbers from the output as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding someNumbers from List of numbers from 1 to 10 gives List(2, 5, 6, 8, 10)\n",
      "Excluding someNumbers from List of numbers from 11 to 20 gives List(11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n"
     ]
    }
   ],
   "source": [
    "val oneToTenRdd = sc.parallelize(1 to 10)\n",
    "val elevenToTwentyRDD = sc.parallelize(11 to 20)\n",
    "println(\"Excluding someNumbers from List of numbers from 1 to 10 gives \" + \n",
    "    oneToTenRdd.filter(x => !someNumbers.contains(x)).collect.toList)\n",
    "    \n",
    "println(\"Excluding someNumbers from List of numbers from 11 to 20 gives \" + \n",
    "    elevenToTwentyRDD.filter(x => !someNumbers.contains(x)).collect.toList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above code works as expected but has some potential performance issues. Imagine that the list ``someNumbers`` is really huge, or perhaps it is some lookup table. In this case for each transformation, the contents of ``someNumbers`` is serialized to all the worker nodes which is expensive and inefficient.\n",
    "\n",
    "Its is ideal in such scenarios to broadcast such frequently read big data structures once to all the workers in the cluster who can use the local versions of this read only data structure in all the transformations executed.\n",
    "\n",
    "Following change is all thats needed to first broadcast the list and then use it across different transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding someNumbers from List of numbers from 1 to 10 gives List(2, 5, 6, 8, 10)\n",
      "Excluding someNumbers from List of numbers from 11 to 20 gives List(11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n"
     ]
    }
   ],
   "source": [
    "val someNumbersBroadcast = sc.broadcast(someNumbers)\n",
    "println(\"Excluding someNumbers from List of numbers from 1 to 10 gives \" + \n",
    "    oneToTenRdd.filter(x => !someNumbersBroadcast.value.contains(x)).collect.toList)\n",
    "    \n",
    "println(\"Excluding someNumbers from List of numbers from 11 to 20 gives \" + \n",
    "    elevenToTwentyRDD.filter(x => !someNumbersBroadcast.value.contains(x)).collect.toList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we see above, the output is identical to the the version where we serialized the list for each transformation performed on the cluster on different RDDs. We first boradcast the variable we wish to read requently and use it in the transformation in the RDD by invoking the ``value`` on the broadcast variable.\n",
    "\n",
    "By default in Java serialization is used in Java and Spark to serialize java variables which is not efficient. To speed up the serialization we may implement our own serialization logic for our datastructure by implementing  ``java.io.Externalizable`` interface or by using an alternate serialization library like Kryo which can be set in spark using the ``spark.serializer`` property. More on property setting later in another notebook.\n",
    "\n",
    "#### Working on per partition basis\n",
    "\n",
    "Certain operations like opening a database connection pool, initializing random number generator etc are operations we dont want to perform per element but we want to do per partition. Operations like ``map`` and ``foreach`` allos us to operatr on per partition basis. Following is an example of per partition operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List((0,1), (8,2), (9,3), (7,4), (5,5), (0,6), (8,7), (9,8), (7,9), (5,10))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneToTenRdd.mapPartitions{\n",
    "    elems =>\n",
    "        val rnd = new scala.util.Random(0)\n",
    "        elems.map(x => (rnd.nextInt(10), x))        \n",
    "}.collect.toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting observation we can make here is that the numbers 1 to 5 probably form a partition and 6 to 10 form another partition. Both of then initialize a random number generator with seed 0 and thus each partition is paired with the generated the random numbers 0, 8, 9, 7 and 5. Similarly we have another method ``mapPartitionsWithIndex`` which provides us with a unique number per partition. As expected we see we have two partitions with index 0 and 1 in the following snippet and each partition generates the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List((0,0,1), (0,8,2), (0,9,3), (0,7,4), (0,5,5), (1,0,6), (1,8,7), (1,9,8), (1,7,9), (1,5,10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneToTenRdd.mapPartitionsWithIndex{\n",
    "    (idx, elems) =>\n",
    "        val rnd = new scala.util.Random(0)\n",
    "        elems.map(x => (idx, rnd.nextInt(10), x))        \n",
    "}.collect.toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Pipe Operations\n",
    "\n",
    "Another interesting operation is the pipe operation which lets us pipe the contents of the RDD to another program and read the results written by the external program back in driver program.\n",
    "For example, suppose we have a shell script called ``testScript.sh`` in current directory as follows\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "while read line\n",
    "do\n",
    "   echo '==='${line}'==='    \n",
    "done\n",
    "\n",
    "```\n",
    "\n",
    "We can pipe the contents of the RDD to the script which is forked by the driven program. The script reads the contents of the RDD from STDIN and writes the contents to STDOUT. This we we can stream the data between two programs. The STDOUT of the script becomes is then captured by Spark to create a new ``PipedRDD``. The performance is not great in such transformation but becomes essential when a complicated logic already implemented in a legacy system which cannot be easily replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List(===1===, ===2===, ===3===, ===4===, ===5===, ===6===, ===7===, ===8===, ===9===, ===10===)\n"
     ]
    }
   ],
   "source": [
    "println(oneToTenRdd.pipe(\"./testScript.sh\").collect.toList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Numeric RDD Operations\n",
    "\n",
    "RDDs allows us to perform gather some numeric statistics on RDDs by making a single pass over the data. Following code snippet shows us some of the statistics available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Count is 10\n",
      "2. Mean is 5.5\n",
      "3. Stdev is 2.8722813232690143\n",
      "4. Max is 10\n",
      "5. Min is 1\n",
      "6. Variance is 8.25\n",
      "7. Sample Variance is 9.166666666666666\n",
      "8. Sample Stdev is 3.0276503540974917\n"
     ]
    }
   ],
   "source": [
    "println(\"1. Count is \" + oneToTenRdd.count())\n",
    "println(\"2. Mean is \" + oneToTenRdd.mean())\n",
    "println(\"3. Stdev is \" + oneToTenRdd.stdev())\n",
    "println(\"4. Max is \" + oneToTenRdd.max())\n",
    "println(\"5. Min is \" + oneToTenRdd.min())\n",
    "println(\"6. Variance is \" + oneToTenRdd.variance())\n",
    "println(\"7. Sample Variance is \" + oneToTenRdd.sampleVariance())\n",
    "println(\"8. Sample Stdev is \" + oneToTenRdd.sampleStdev())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All stats above are self explanatory with the Sample Variance and Sample Stdev taken with the denominator as N - 1 instead of N. The following code snippets computes these two stats. The Stdev is simply square root of these stats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance is 8.25 Sample Variance is 9.166666666666666\n"
     ]
    }
   ],
   "source": [
    "val mean = oneToTenRdd.mean()\n",
    "val count = oneToTenRdd.count()\n",
    "val nr = oneToTenRdd.map(x => (x - mean) * (x - mean)).sum()\n",
    "println(\"Variance is \" + nr / count + \" Sample Variance is \" + nr / (count - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

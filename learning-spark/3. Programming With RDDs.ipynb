{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming With RDDs\n",
    "\n",
    "In this notebook we will be experimenting with RDDs. \n",
    "\n",
    "RDD stands for Resilient Distributed Dataset. They form the core abstraction of Spark which allows us to perform distributed operations on the data. All transformations gives us new RDDs and we invoke few actions to trigger some evaluation and execution of the transformation on the data.\n",
    "\n",
    "Let us first create RDD from an online page and filter some text from the its lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class of linesRDD is class org.apache.spark.rdd.ParallelCollectionRDD\n",
      "Class of linesRDD is class org.apache.spark.rdd.MapPartitionsRDD\n"
     ]
    }
   ],
   "source": [
    "val content = scala.io.Source.fromURL(\"https://raw.githubusercontent.com/apache/spark/master/README.md\")\n",
    "val strContent = content.mkString\n",
    "val lines = strContent.split(\"\\n\")\n",
    "val linesRDD = sc.parallelize(lines)\n",
    "println(\"Class of linesRDD is \" + linesRDD.getClass)\n",
    "val filtered = linesRDD.filter(_.toLowerCase.contains(\"scala\"))\n",
    "println(\"Class of linesRDD is \" + filtered.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, ``filtered`` isn't really the filtered values but another RDD. the ``filter`` operation is a transformation which gives us another RDD. Transformations are lazy. When invoked on another RDD they give us another RDD keeping a track of all the transformations applied. We will now invoke an action to execute these transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high-level APIs in Scala, Java, Python, and R, and an optimized engine that"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoking ``first`` on the filtered RDD gives us the first matching line in the RDD which contains the word \"scala\". Having lazy evaluation on transformations and defering them till we perform an action (method `first` in this case) gives the framework chance to optimize the execution. In this case, since the action if ``first``, Spark can stop execution as soon as the first match is found. Transformations like ``filter`` may reduces the amount of data passed to the next transformation/action and thus spark can efficiently load the data from the underlying source.\n",
    "\n",
    "It is important to note that each time an action is performed on an RDD, all the transformations are applied. For our trivial case its probably ok. But when loading the data from a datasource and performing transformation on them is time consuming, caching the RDD might be a good idea. By default caching is not done as it can be wasteful of the RDD is not needed in future. However, systems that cache frequently used RDDs will give better performance. Caching can be done either in memory or to disk and is done using the ``cache`` of ``persist`` method (we will see them in use later). \n",
    "\n",
    "As we see below ``cache`` or ``persist`` is not an action but transformation. We simily express our interest to cache the contents the first time the an action is called triggering the evaluation of the transformations. If the value of the RDD (or a part of it) is cached after first evaluation, subsequent executions will no longer need to execute the RDD again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class if cachedRDD is class org.apache.spark.rdd.MapPartitionsRDD\n"
     ]
    }
   ],
   "source": [
    "val cachedRDD = filtered.cache;\n",
    "println(\"Class if cachedRDD is \" + cachedRDD.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following line of code counts the number of words in the file README.md we read from the source. Note that we consider a space as a delimiter for our example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD.map(_.split(\" \") match {\n",
    "    case Array(\"\") => 0\n",
    "    case x  => x.length\n",
    "}).reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Downloading Spark and Getting Started\n",
    "\n",
    "This notebook is about how to going with spark from Jupyter notebook. Its extremely simple and has the follwing few steps\n",
    "\n",
    "- Download and install Docker\n",
    "- Run the command ``docker run -it --rm -p 8888:8888 jupyter/all-spark-notebook``. This will download the docker image which comes with preinstalled Spark 2.2, Jupyter and many other libraries. All details of this image can be found [here](https://github.com/jupyter/docker-stacks/tree/master/all-spark-notebook). First time it will take time to download the image but its pretty simple going forward. You don't need to be an expert in Docker but a few commands will be helpful. Refer [this](https://github.com/wsargent/docker-cheat-sheet) cheat sheet.\n",
    "- With docker container running, simply copy the URL displayed in the browser and start an ``Apache Toree - Scala`` sheet.\n",
    "\n",
    "If we successfully print the variable ``sc`` in the notebook as below, then congratulations, we have a Jupyter notebook running with a Spark context variable available to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkContext@9a99a55"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have the README.md locally on the docker container, let's download it from github, remember we need internet connectivity while running this command. Also, we convert the return value ``html`` of type ``scala.io.BufferedSource`` to a ``string``. We then split the string by new line to an array and create an RDD from the Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Line is `# Apache Spark`\n",
      "Number of lines are 103\n",
      "the variable line is of type class org.apache.spark.rdd.ParallelCollectionRDD\n"
     ]
    }
   ],
   "source": [
    "val html = scala.io.Source.fromURL(\"https://raw.githubusercontent.com/apache/spark/master/README.md\")\n",
    "val readMeString = html.mkString\n",
    "val lines = sc.parallelize(readMeString.split(\"\\n\"))\n",
    "println(\"First Line is `\" + lines.first + \"`\")\n",
    "println(\"Number of lines are \" + lines.count)\n",
    "println(\"the variable line is of type \" + lines.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequent chapters will use a new notebook per chapter to demonstrate the concepts shown by the chapter. We will not bother how the variable ``sc`` is constructed but use one that available. This is similar to working on the spark-shell which is good to experiment with small data in standalone mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
